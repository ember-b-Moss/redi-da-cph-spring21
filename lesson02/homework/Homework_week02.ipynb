{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "\n",
    "This week, there are two homework tasks, of which the web scraping task is optional. You will use web scraping only if you can't retrieve the data otherwise. Because the internet is messy and all websites are different, scripting a web scraping tool can be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Task\n",
    "\n",
    "1. Go to https://spoonacular.com/food-api and get familiar with the information you can get from this API. Find out how to authenticate a request to this API.\n",
    "2. Retrieve 150 random recipes\n",
    "4. Create a .csv file with the following headers:\n",
    "- vegetarian\n",
    "- glutenFree\n",
    "- dairyFree\n",
    "- veryHealthy\n",
    "- healthScore\n",
    "- aggregateLikes\n",
    "- id\n",
    "- title\n",
    "- pricePerServing\n",
    "- readyInMinutes\n",
    "- servings\n",
    "- sourceUrl\n",
    "- there is a list of nutrients, create two columns per nutrient: one stating the amount and one the percentOfDailyNeeds. The name is e.g. \"seleniumAmount\" and \"seleniumPercentOfDailyNeed\"\n",
    "\n",
    "**Handin:**\n",
    "- your notebook (.ipynb file)\n",
    "- your .csv file\n",
    "(Can also be a link to a GitHub folder containing both files)\n",
    "\n",
    "**Hints:**\n",
    "- In order to retrieve the information you are looking for, you need to add a query to the request url, which is described in the API documentation\n",
    "- Please be aware, that the use of this API is restricted. The documentation states that you have a certain amount of API calls each day. You see your remaining points in the response header. Keep an eye on it when developing your script. \n",
    "- To avoid repeated calls to an API, use another cell in your notebook, once you stored the information in a variable. This is one thing that makes jupyter notebooks very convenient.\n",
    "- Keep the .csv file, we might want to work with it in the future, but don't worry if you didn't manage, we'll provide you with a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional task: Web Scraping\n",
    "We want to create a list of all currently open data analytics jobs in Copenhagen:\n",
    "- job title\n",
    "- company\n",
    "- link to the job advert\n",
    "- publication data\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage https://www.jobindex.dk/\n",
    "\n",
    "The information is available here: https://www.jobindex.dk/jobsoegning/storkoebenhavn?q=%27Data+Analyst%27\n",
    "\n",
    "Handin:\n",
    "- your jupyter notebook\n",
    "- .csv file with the information\n",
    "\n",
    "**Hints** \n",
    "- Make sure to make use of the developer tools of your browser to inspect the website you are scraping and find the tags you need.\n",
    "- Make your scraper robust, so that it does not crash in case of unexpectet values in a record."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
